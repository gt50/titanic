{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Learning Linear Regresison\n",
    "\n",
    "This example uses linear regression with batch gradient descent to estimate house prices, based on a small training set.   \n",
    "\n",
    "In general our goal with linear regression is to fit a linear function to our features so that we can learn some function $f(x)$ that can predict a dependent variable based on a set of independent variables, by learning 'weights' .  The general form of the function we're trying to fit is:\n",
    "\n",
    "$$f(x) = \\theta_{0} + \\theta_{1}*X_{1} + \\theta_{2}*X_{2} + â€¦ + \\theta_{n}*X_{n}$$\n",
    "\n",
    "As a concrete example, this demonstration will attempt to predict the dependent variable 'house price' based on the independent variables '# of bedrooms' and 'square feet' by finding some values for $\\theta_{0},\\theta_{1},$ and $\\theta_{2}$ that make the function $f(x) = \\theta_{0}+\\theta_{1} * 'bedrooms' + \\theta_{2} * 'squareFeet'$ as accurate as possible based on some training examples.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Data\n",
    "\n",
    "\n",
    "\n",
    "When solving multivariate linear regression with an iterative method the training data should be scaled (aka normalized, aka standardized) so that all the features are on a similar scale.  Said simply, it helps to have the values of each feature be close in size.   This scaling step is not typically done when solving linear regression directly or with [MLE](http://en.wikipedia.org/wiki/Maximum_likelihood).\n",
    "\n",
    "In order to scale our data, we'll subtract the feature mean from each value and divide by the feature standard deviation.  This will give our data a mean of 0 and a standard deviation of 1.  The scaleData function below implements this procedure, known as [z-score or standardized score in statistics](http://en.wikipedia.org/wiki/Standard_score).  \n",
    "\n",
    "$$z = \\frac{X - \\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#z-scores some array\n",
    "def scaleData(z):\n",
    "   mu = z.mean(axis=0)\n",
    "   sigma = z.std(axis=0)\n",
    "   z = (z - mu)/sigma \n",
    "   return z, mu, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#My sample data set to train our regression with.   \n",
    "#This is an array with features price, bedrooms, and square footage\n",
    "#For example a 100,000 dollar house has 2 bedrooms and 1600 square feet.\n",
    "\n",
    "trainData = np.array([[100000,2,1600],\n",
    "                     [200000,4,2500],\n",
    "                     [250000,4,3000],\n",
    "                     [150000,3,2000]])\n",
    "#We'll scale the data as noted above.\n",
    "trainData, mu, sigma = scaleData(trainData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we're going to split our array into y, the variable we want to predict (house price)\n",
    "and X, the array of independent variables we're going to train on.\n",
    "\n",
    "If you recall from above, our linear equation included a contant $\\theta_{0}$ that isn't present in our training data X, so we'll need to add it.   By convention we set $\\theta_{0}$ to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.matrix(trainData[:,0]) #slice the first column, house price\n",
    "y = y.T #we just prefer y to be a column vector instead of a row vector.\n",
    "X = np.matrix(trainData[:,1:]) #slice the rest of the column into matrix X\n",
    "\n",
    "#get the number of training samples in X\n",
    "m = y.size\n",
    "#Add a column of ones, size m, to X (interception data)\n",
    "it = np.ones(shape=(m, 1))\n",
    "X = np.append(it,X,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Cost Function\n",
    "\n",
    "The purpose of this function is to show us how accurate our $\\theta$s are.  Our goal is to minimze this fucntion.  When our model is very accurate (the predicted value is very close to the actual value), J will become very small.   \n",
    "\n",
    "In a machine learning linear regression model, the cost is typically defined as:\n",
    "$$J = \\frac{1}{2m} \\sum_{i=1}^{m}( \\hat{y} - y)^2 $$   This is a slightly modified sum of squared error function, where we're scaling the value based on the number of features.   Note that y is the actual value in our training data and $\\hat{y}$ is the predicted value of y, based on the current $\\theta$s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluate the linear regression\n",
    "def compute_cost(X, y, theta):\n",
    "    m = y.size\n",
    "    y_hat = X.dot(theta)\n",
    "    J = (1.0/2*m)* (y_hat - y).T.dot((y_hat - y))  \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Note: Math Tricks\n",
    "\n",
    "Note that while I could have used loops to handle the summation of each row in the training set (i.e. for each row in X, compute the error and add that together), I'm instead using matrix multiplication, to handle the calculation all at once.   This is an important optimization, and one that is used often in ML.  Never loop when you can use linear algebra to vectorize your operations.\n",
    "\n",
    "For example, if I want to predict the values of X based on my current thetas as above, I can multiple my training data set X, a 4x3 matrix with  $\\theta$, a 3x1 vector to result in a 4x1 vector of predicted values.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Now that we have a cost function that can estimate the correctness of our current values of $\\theta$, all we have to do is minimize that error rate J by picking the best possible values of $\\theta$ we can based on our training data.\n",
    "\n",
    "To do that, we'll use an optimization algorithm known as 'batch gradient descent.'\n",
    "\n",
    "On the surface, batch gradient descent is pretty easy:\n",
    "\n",
    "Repeat Until Converged: {\n",
    "$$\\theta:=\\theta-\\alpha\\frac{\\partial}{\\partial\\theta}J(\\theta)$$\n",
    "}\n",
    "\n",
    "So, every time we update theta, we will set theta equal to the previous value minus $\\alpha$ (a learning rate) multiplied by the partial derivative of the cost function J, with respect to $\\theta$\n",
    "\n",
    "By doing the caluclus, we can get to\n",
    "$$\\frac{\\partial}{\\partial\\theta}J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m}( \\hat{y} - y)^2 $$\n",
    "\n",
    "So then:\n",
    "\n",
    "$$\\theta:=\\theta-\\alpha\\frac{\\partial}{\\partial\\theta}J(\\theta)$$\n",
    "Can be expanded to become:\n",
    "$$\\theta := \\theta + \\alpha \\frac{1}{2m} \\sum_{i=1}^{m}( y^{i} - \\hat{y}^{i} )^2 x^{i}$$\n",
    "Which is what we've implemented below.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "   \n",
    "    m = y.size\n",
    "    J_history = np.zeros(shape=(num_iters, 1)) # a column vec to hold our previous Js\n",
    " \n",
    "    for i in range(num_iters):\n",
    "        gradient = (1.0/2*m) * (( y - X.dot(theta)).T.dot(X)).T\n",
    "        theta =  theta + (alpha * gradient)\n",
    "        J_history[i, 0] = compute_cost(X, y, theta)\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding our $\\theta$s\n",
    "\n",
    "Now that we've implemented these functions, we can pass X (our training data), y (our dependat variable, house price), alpha, and iterations to the gradient_descent function to learn values for $\\theta$\n",
    "\n",
    "We've set $\\alpha$ to .01, which is a commonly used good start.  We've set iterations to 1000, which should be more than enough in this case.  A more sophisticated approach would be to repeat until J stops changing significantly (i.e. converges)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-6-e9640bb4c240>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-e9640bb4c240>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    print theta\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "alpha = .01\n",
    "iterations = 1000\n",
    "#np.shape[1] is the number of features, so we need an equal # of thetas\n",
    "theta = np.zeros((X.shape[1],1))\n",
    "\n",
    "theta, J_history = gradient_descent(X,y,theta,alpha, iterations)\n",
    "print theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Tuning Gradient Descent\n",
    "\n",
    "We can adjust $\\alpha$, the learning rate, and iterations, the number of times gradient descent loops to make gradient descent run better.   Let's try some different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "def plot_grad_descent(alpha, iterNum):\n",
    "    theta = np.zeros((X.shape[1],1))#reinitialize theta\n",
    "    theta, J_history = gradient_descent(X, y,theta, alpha, iterNum)\n",
    "    plot(J_history)\n",
    "    title(\"alpha = \" +str(alpha)+ \"; iterations = \" +str(iterNum))\n",
    "    show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_grad_descent(.01,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph we can see that $J(\\theta)$ is decreasing as iterations increases, which is expected.   While $J(\\theta)$ is beginning to stablize, we probably need more iterations before it becomes stable.   Lets try 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_grad_descent(.01,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph we can see $J(\\theta)$ has become very small and begins to approach 0 asymptotically as $J(\\theta)$ approaches infinity.  Lets further prove that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_grad_descent(.01,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see $J(\\theta)$ over 1000 iterations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEICAYAAAC55kg0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAIABJREFUeJzt3XtYlGX6wPHveBYxNEUsUFHcMBMVT4mpYFlUmOGWeUjF\n",
       "POS2ZXbYYrdy09pSt8zCShFNyRJ/tSXiAUvWEC0UNTUNxUJMUQJSQAQUgef3x5OzkqADDrzDzP25\n",
       "rrku5z3N/YK89zxnk1JKIYQQwiHVMzoAIYQQxpEkIIQQDkySgBBCODBJAkII4cAkCQghhAOTJCCE\n",
       "EA5MkoCdWrFiBYMGDbL6seLq7r//flauXGloDM2bN+fYsWOGxiDqDkkCwmYsXbqUW265hZtvvpkZ\n",
       "M2ZQWlpa6bGPP/44Xbp0oX79+kRGRpbbFxkZSZ8+fXBxccHDw4PQ0NBy1zp06BB33nknLVq0wNfX\n",
       "l//+97/mfceOHaNevXo0b97c/HrjjTcsvoeNGzcyfvx4oHaSa0BAAMuWLSu3LT8/H09Pzxr9XGv4\n",
       "5ptvGDJkCC1atKBjx45X7B8yZAht2rShVatW3HvvvaxYsaL2g3QAkgSETUhISOCVV15h7dq1JCcn\n",
       "88MPP/Dmm29WenzPnj358MMP6dWrFyaTqdy+oqIi3nvvPU6fPs369etZs2YNb7/9NgAlJSU8+OCD\n",
       "9OvXj2PHjvG3v/2NoKAgTp48We4aZ8+eJT8/n/z8fF5++WXr37AFSkpKrnnMH++9LnF2dmbKlCm8\n",
       "9dZbFe4PCwvj5MmTZGZmMn36dKZMmUJKSkotR+kAlKiz5syZo7y8vJSzs7Pq2rWrWrNmjXnf8uXL\n",
       "1cCBA83vTSaTWr58uerRo4dydXVVL7zwgiorKyt37OzZs9XNN9+sOnbsqGJjY83nfvTRR+rWW29V\n",
       "zs7OqlOnTio8PNzq9xISEqJefvll8/stW7ao9u3bX/O8gQMHqsjIyKse869//Us98MADSimlDhw4\n",
       "oBo1aqTOnj1r3u/r66tef/11pZRSaWlpymQyqZKSkgqv9emnn6ru3btX+ln+/v5q6dKl6tChQ6px\n",
       "48aqfv36ytnZWbVs2VIppdT58+fV888/r9q3b6/atGmj/vKXv6iioiKllFLffPONcnd3V4sWLVJ/\n",
       "+tOf1IQJE1ROTo4KCgpSrq6uqkWLFmrYsGEqPT1dKaXUSy+9pOrXr6+aNGminJ2d1fTp05VS+ned\n",
       "mpqqlFIqNzdXjR8/Xrm6uqoOHTqof/3rX+V+73fccUelv/fo6GgVEBCgXFxcVMeOHdWnn3561Z9z\n",
       "dW3evFl5enpWuv/ChQsqNjZWtWrVSuXn59dIDI5MSgJ1WOfOndm+fTtZWVlMmTKFRx99lMzMzEqP\n",
       "j4yMJD4+nj179vDll1/y0UcfmfclJSUBcPDgQSZMmMDkyZPN+9zc3NiwYQNnzpzhjTfe4K9//St7\n",
       "9+6t8DO2b99Oy5YtK3199913FZ535MgRevToYX7fvXt3Tpw4wfnz56v0M6lIYmJiuWsDlJWVmf9d\n",
       "UlJyxTfMDh060K5dOyZNmsTp06fN28eOHcv+/fsr/SyTyYTJZKJLly6Eh4fj5+dHfn4+Z86cAeDv\n",
       "f/87+/btY+PGjXz33XccPnyY1157zXx+ZmYmu3btIiEhgfDwcMrKypg8eTLHjx9n9+7dXLx4kaee\n",
       "egqAN954g0GDBvHBBx+Qn59PWFjYFfFMnz6dnJwc0tLS2Lp1K8uXL2f58uXm/bt27QKu/L1fvHiR\n",
       "GTNmMHfuXHJzc0lMTKRnz54V3vOqVasq/X3feOONpKenV/rzupZhw4bRvHlzRo0axZYtW3B2dq72\n",
       "tUQljM5CwjrKyspUu3btVExMjFKq4pLAsmXLzO//8Y9/qLvuust8bMuWLVVpaalSSqlTp04pk8mk\n",
       "MjMzK/ysgQMHqvfee8+q8d9yyy3qq6++Mr8vLi5WJpPJ/K23MtcqCSxZskS1b9/e/M3/4sWLqlOn\n",
       "Tur5559XWVlZavny5apevXrqvvvuU0opde7cObVnzx5VWlqqdu3ape655x4VGBho8X0EBASYf85/\n",
       "/B2UlZWpZs2aqW+//da8bc2aNapjx45KKV0SMJlM6vjx45Vef/PmzeZSxaXPW7p0abljLpUESkpK\n",
       "VKNGjcr9XF9++WUVEBBgjq+y3/vFixfVTTfdpD755BNVUFBg8f1Xx7VKArm5uSosLEy1bt1a/fbb\n",
       "bzUaiyOSkkAd9vHHH9OzZ0/zN66MjAx+++23So+//Jucr68vp06dMr+/7bbbqFdP/3e46aabADh3\n",
       "7hwAsbGx9O/fn1atWtGyZUuSkpLKfTu2hlatWnH27Fnz+7y8PPP26oqOjmbmzJnExsbSvHlzABo0\n",
       "aEB0dDS7d++mW7duREdHM3bsWNzd3QFo1qwZvXr1ol69evTp04c5c+bw9ddfU1BQcB13p2VnZ1NY\n",
       "WEhQUJD5m/LEiRPL/c7c3Nxo166d+X1hYSHTpk3D09MTFxcXHnroIfLy8lCXzftYWbvAb7/9xsWL\n",
       "F+ndu7d5W+/evcu1f1T2e2/QoAFffPEF//nPf3B3d2fYsGH89NNP1/0zqA4XFxemT59Ou3bt2Lhx\n",
       "oyEx2DNJAnXUL7/8wqRJk5gzZw5nzpzhzJkzuLu7l3s4/NHlVTjff/+9+cF3NRcuXOChhx5iypQp\n",
       "ZGVlkZOTQ79+/Sr9nG3btpXrWfPH17ffflvhed7e3uzbt8/8fv/+/bRr144mTZpcM8aKfPXVVzz+\n",
       "+OOsX7+erl27ltvn4+NDfHw8mZmZREdHk5qaSr9+/Sq8zqX7vLz6yFImk6ncz6l169Y0bdqUr776\n",
       "ipycHHJycsjNzS2X/Bo0aFDuGvPnz2fHjh3s3LmTvLw8vvjiC5RS5uv+8TMu17p1axo2bMju3bvN\n",
       "23bv3o2Hh4dF8fv5+bFmzRoyMjLo0KEDL730UoXHffrpp5X+vm+44Ybrqg66XGFhIRcuXLDKtcT/\n",
       "SBKooxo3bkzjxo1p1aoVeXl5zJkzp9w3+4qsWrWK3NxcTpw4wX/+8x9GjRpl0Wc5OzvTqlUrzp8/\n",
       "z/Lly9m5c2elxw4aNMjcq6ai1x133FHheZMmTWLZsmUcOnSInJwcXn/9daZOnVrp51y8eJHz589T\n",
       "VlZGcXEx58+fNz8Mv/nmG8aOHcuXX35Jnz59rjj3wIEDnD9/nsLCQt5++22ys7OZOHEioNtGUlJS\n",
       "KCsrY9++fbz88ssMGTLEXJJYsWJFhd0ZK+Lm5sZPP/1kLlHVq1ePqVOn8s9//pPvv/+esrIyTp48\n",
       "yddff13pNZydnWnRogWNGzcmOTmZefPmXfEZ33//fYWJoH79+jzyyCN8+OGHnDt3jl9++YXPPvuM\n",
       "cePGXTP2rKws1q5dS0FBAaWlpTRs2LDSUuajjz5a6e/77NmzlSYdpRTnz5/n4sWLKKW4cOECxcXF\n",
       "AKSkpBAbG0tRURG//vor//73vzl16hT33HPPNWMXVSNJoI5q27Ytc+bMYfz48fTo0YPi4mIGDhxo\n",
       "3n+pgfJyEyZMwN/fn169ehEcHGxuBKzo2EvvGzduzHvvvcdrr71G586d2b17N6NHj7b6/QwaNIjX\n",
       "X3+d4cOHc9ttt9GjR49y3zzvv/9+5s6da35/99134+TkxI4dO3j88cdxcnJi27ZtALz++uvk5+dz\n",
       "3333mb+RBgUFmc9duXIlN998M506dWLHjh1s376dhg0bAnD06FHzeffffz9t27YlKirKfO6JEyfK\n",
       "/Zyv5q677qJv3754eHjQpk0bAObNm0fv3r15+OGHadGiBXfffTdHjhwxn/PH38OkSZNwd3fnlltu\n",
       "Yfz48UyaNKncMTNmzCAuLo4WLVrwzDPPXBHDwoULueGGG+jUqRODBg0iJCSExx57zPxZlf3ey8rK\n",
       "WLBgAe7u7nTp0oUzZ86wePFii+7bUlu3bsXJyYmgoCBOnDhB06ZNuffeewGdIGbPno2bmxtdu3Zl\n",
       "48aNxMbG0r59e6vGIMCkrlZ/IOxGvXr1+Pnnn+nUqZPRodRpgYGBhIWF4e3tbXQoQljFVUsCKSkp\n",
       "+Pr6ml8uLi5XdEPLz88nODgYLy8v/Pz8SE1NNe97//336dSpE7fddhtLly694pzu3bszYsQIc3FZ\n",
       "CFv31VdfSQIQduWqScDb25u9e/eyd+9e9uzZg5OTEyNGjCh3TFRUFO3btyc1NZV33nnHXNTMy8tj\n",
       "4cKF7Nmzh507d7JkyRJzgnj99dcZMGAAP/zwA/379+df//pXDd2euKQujywVQtQci9sE4uLi8PLy\n",
       "Ktd9DWDLli3m+lY/Pz+OHDlCVlYW3333Hb169aJly5Y4OzszZMgQvvzySwBiYmIICQkBICQkhOjo\n",
       "aGvdj6hEaWmpVAUJIa5gcRJYvXo1Y8eOvWJ7YGAgUVFRFBUVERMTQ1ZWFidPnmTw4MEkJSWRlpZG\n",
       "RkYGGzduNHcVy8zMxM3NDdC9G642ylUIIUTNaXDtQ6C4uJh169Zd0T0NYNSoUaSnp+Pv74+3tzd9\n",
       "+vShfv36NGvWjHfffZcnn3ySvLw8Bg0aRP369a84v6IeCpe2CyGEqLoq9fexZFhxdHS0xUPnPT09\n",
       "K5zk6R//+IdatGiRUkopb29vlZGRoZTSQ9W9vb2vON7C0OqsV1991egQapTcX91mz/dnz/emVNWf\n",
       "nRZVB0VFRTFmzJgK9+Xl5ZkHeERERODv72+e5CkrKwuA48ePs2bNGnN10vDhw81zwEdGRhIcHGx5\n",
       "1hJCCGE116wOKigoIC4ujoiICPO28PBwAKZNm0ZycjITJ07EyckJHx+fcgNKHn74YXJzc3F2diYy\n",
       "MpIbbrgBgJkzZzJ+/Hi6d++Ol5eX4SsxCSGEo7LZwWJXmxPFHsTHxxMQEGB0GDVG7q9us+f7s+d7\n",
       "g6o/OyUJCCGEHanqs1PmDhJCCAcmSUAIIRyYJAEhhHBgkgSEEMKBSRIQQggHJklACCEcmCQBIYRw\n",
       "YJIEhBDCgUkSEEIIByZJQAghHJgkASGEcGCSBIQQwoFJEhBCCAcmSUAIIexESUnVz5EkIIQQdqKC\n",
       "ZeCvSdYTEEIIO7BvH9x9N/z2mx2tJyA5QAghrq24GEJC4O23q36uTSeB+HijIxBCCNv32mvg6QkT\n",
       "JlT9XJtOAu+/b3QEQghh25KSICICwsPBZKr6+TadBOLj4fhxo6MQQgjbVFSkq4EWLoS2bat3DZtO\n",
       "AuPHw+LFRkchhBC26ZVXoHt3eOSR6l/DpnsHpaQoBg7UpYEmTYyOSAghbMe2bTBqFPzwA7Ru/b/t\n",
       "Ve1ZedWSQEpKCr6+vuaXi4sLYWFh5Y7Jz88nODgYLy8v/Pz8SE1NNe+LiIhgwIAB9O7dm2eeeca8\n",
       "fdasWXh4eJivu2nTpgo//5ZboHdvWL3a4vsRQgi7d+4cTJyoa0ouTwDVoixUWlqq2rZtq44fP15u\n",
       "e3h4uJo+fbpSSqnvvvtODRo0SCml1OnTp5Wnp6c6d+6cKi0tVffdd5/atGmTUkqpWbNmqfnz51/1\n",
       "8y6Ftn69Ur16KVVWZmmkQghh3554QqmQkIr3VeGxrpRSyuI2gbi4OLy8vGjXrl257Vu2bCEoKAgA\n",
       "Pz8/jhw5QnZ2Nk2bNkUpRV5eHkVFRRQWFtKyZcvLk49Fn3vffZCXBzt2WBqpEELYr82bYf16ePdd\n",
       "61yvgaUHrl69mrFjx16xPTAwkKioKAYPHszmzZvJysoiPT0dX19fFi1ahKenJ40bN+bpp5+mX79+\n",
       "5vMWLlzIsmXL8PPzY/78+bRo0eKKa8+aNQuAzp1h5swA4uICqn6HQghhJ/LyYPJkWLoULj0y4+Pj\n",
       "ib+OQVUWNQwXFxfj7u5OcnIyrq6u5fYVFhYyf/581q1bh7e3NykpKSxdupSbbrqJvn37EhcXR8uW\n",
       "LRk5ciTPP/88QUFBZGVl4erqytmzZ3nhhRcoLS1l2bJl5QO7rHEjNxc6doRDh6rfDUoIIeq6xx7T\n",
       "nWQWLar8GKs2DF8SGxtL7969r0gAAE5OTsycOZOkpCRWrlxJdnY2nTp1Iikpif79+9O5c2datWrF\n",
       "yJEjSUhIAKBNmzaYTCZcXFx48sknSUpKuurnt2ihW8GXLLH4voQQwq6sWwcJCfDWW9a9rkVJICoq\n",
       "ijFjxlS4Ly8vj+LiYkD3BvL398fZ2ZmBAweye/duzpw5w4ULF4iNjeWee+4BICMjA4CSkhJWrVqF\n",
       "j4/PNWN48kndEv77RwkhhMM4fRqmTYPly8HZ2brXvmYSKCgoIC4ujj//+c/mbeHh4YSHhwOQnJyM\n",
       "j48Pvr6+bNu2jQ8//BAAFxcXXnnlFUaMGMHAgQPp0aMHd955JwChoaF0796d/v37c/HiRRYsWHDN\n",
       "QH18wNsb1qyp1n0KIUSd9eSTMHo0DB5s/Wvb9GCxP4b2xRewYAFs325QUEIIUcs++wz++U/Yuxea\n",
       "Nr328VVtE6hTSaCkBDp1grVrwdfXoMCEEKKWZGZCjx76mXf77ZadUyMNw7aiQQN44gmZXVQIYf+U\n",
       "gscf111CLU0A1VGnSgIA2dl6Oomff4ZWrQwITAghasHHH+tFYnbtgsaNLT/PrksCAK6u8OCD8Idh\n",
       "BUIIYTfS0+Fvf9OJoCoJoDrqXEkAYPduePhhSE2F+vVrOTAhhKhBSsG998KgQXqq6Kqy+5IAQJ8+\n",
       "cNNNev4MIYSwJ0uWwJkz8Pe/187n1cmSAMCnn+qBE3FxtRiUEELUoKNHdSPw1q3QtWv1rmHXXUQv\n",
       "V1wMHTrAf/9b/R+WEELYirIyGDIEhg+H55+v/nUcojoIoFEj3X3qgw+MjkQIIa5fWBiUlsJl62/V\n",
       "ijpbEgA4dQq6dYO0NHBxqaXAhBDCylJSYOBASEzUU+dfD4cpCQDcfDPccw9ERhodiRBCVE9JCYSE\n",
       "wKxZ158AqqNOJwGAp57SI4jLyoyORAghqu7tt/XMoE88Yczn1/kkcMcd0KyZXnJNCCHqkgMHYP58\n",
       "+OgjqGfQ07jOJwGTSZcGFi40OhIhhLBccTFMmADz5kH79sbFUacbhi8pKtI/xB07wMurhgMTQggr\n",
       "ePVV2LNHrxhmMlnvug4zTuCPQkN1A8v8+TUYlBBCWMHu3RAUBPv26dkPrMlhk8CxY3o6iV9+0W0E\n",
       "Qghhi86fh169YOZMqGTV3uviUF1EL+fpqfvZfvqp0ZEIIUTl/vlPuO02vVykLbCbkgDoeYSefRZ+\n",
       "+MG6dWxCCGEN336rZ0D+4Qc9LX5NcNiSAMBdd+l2gYQEoyMRQojyCgpg4kRYtKjmEkB12FUSkO6i\n",
       "Qghb9fe/g58fBAcbHUl5dlUdBJCfr9sH9u2Ddu2sH5cQQlTVf/+rSwE//AAtW9bsZ1m1OiglJQVf\n",
       "X1/zy8XFhbCwsHLH5OfnExwcjJeXF35+fqSmppr3RUREMGDAAHr37s0zl02Nd+mc7t27M2LECM6d\n",
       "O2dxwNfSvDmMGweLF1vtkkIIUW1nz+rF4iMiaj4BVIfFJYGysjLc3d1JSkqi3WVfsZcsWcLBgwcJ\n",
       "CwsjMTGR0NBQEhISOHPmDL179+bgwYM0bdqUYcOGMWPGDAIDA3nxxRdp3bo1L774IvPmzSMnJ4e5\n",
       "c+eWD6yaJQGAI0f00my//AJNmlTrEkIIYRVTpugpIZYsqZ3Pq7GG4bi4OLy8vMolAIAtW7YQFBQE\n",
       "gJ+fH0eOHCE7O5umTZuilCIvL4+ioiIKCwtp+XsajImJISQkBICQkBCio6MtDtgSt9wCvr7w2WdW\n",
       "vawQQlRJdLSuCrLlQawWJ4HVq1czduzYK7YHBgYSFRVFUVERMTExZGVlkZ6eTtOmTVm0aBGenp60\n",
       "bduWO+64g379+gGQmZmJm5sbAG5ubmRmZlrpdv7nUgOxbbZ4CCHsXVqaXvhq9WpdTW2rGlhyUHFx\n",
       "MevWrWPevHlX7Bs1ahTp6en4+/vj7e1Nnz59qF+/PtnZ2TzxxBMkJyfTsmVLRo4cyYYNG8ylhktM\n",
       "JhOmSjr1z5o1y/zvgIAAAgICLL6x++6DGTMgKUmv2SmEELXlwgV45BF46aWaf/7Ex8cTHx9f7fMt\n",
       "ahNYu3YtixYtYtOmTde8YMeOHTlw4ABbt25l5cqVrF69GoBFixbxyy+/MHfuXLp06UJ8fDxt27Yl\n",
       "IyODIUOGcPjw4fKBXUebwCXz58PevfDJJ9d1GSGEqJLp0+HkSfjii9ofuFojbQJRUVGMqWSSi7y8\n",
       "PIqLiwHdG8jf3x9nZ2cGDhzI7t27OXPmDBcuXCA2Npa7774bgOHDhxP5+3JgkZGRBNdQx9lJk2DD\n",
       "BqiB2iYhhKjQ55/Dxo16jYC6MHPBNUsCBQUFdOjQgbS0NJr/XrEVHh4OwLRp00hMTGTixIk4OTnh\n",
       "4+PD4sWLcXJyAmDFihUsX76cwsJC7r33XmbPnk29evXIz89n/PjxHD16FC8vL1auXImzs3P5wKxQ\n",
       "EtAxgoeHnqxJCCFq0k8/6YWuYmOhd29jYnDYWUQrc+AA3HuvnmW0YcPrj0sIISpSVAQDBsDUqfDX\n",
       "vxoXh0PPHVQRHx/4059gzRqjIxFC2LNnngFvb+PWCq4uu08CIPMJCSFq1qefwjff6AFhdaEd4HJ2\n",
       "Xx0EembRjh31Mm49e1rlkkIIAcDhw3qGgrg46NHD6GikOqhCDRrAX/4C779vdCRCCHtSWKjXB5gz\n",
       "xzYSQHU4REkAICtL19elpsKNN1rtskIIB/bYY7qm4eOPbacaSEoClWjTBoYPh2XLjI5ECGEPVqyA\n",
       "nTv1IjG2kgCqw2FKAgC7dumh3D//DPXrW/XSQggHcvAgDBkC8fF6vWBbIiWBq+jbF9zc9ChiIYSo\n",
       "jnPnYORIePtt20sA1eFQJQHQ8whFRsLmzVa/tBDCzimlF61q0sR2q5alJHANI0fqUcSHDhkdiRCi\n",
       "romI0M8Pexp35HAlAdDzCOXkSJdRIYTl9u2Du++G7dt1T0NbJXMHWeDkST2dxLFjcMMNNfIRQgg7\n",
       "cvasnhDutdegkgmVbYZUB1nA3R2GDtVtA0IIcTVK6XWChw61/QRQHQ6ZBEAv+vD++1BWZnQkQghb\n",
       "9sEHulv5ggVGR1IzHDYJDBwITZvq+T6EEKIiu3bpKqDPP9c9guyRwyYBk0lmFxVCVC4nB0aN0iOC\n",
       "vbyMjqbmOGTD8CWFhdC+vV6MvlOnGv0oIUQdohSMGAEdOsB77xkdTdVIw3AVODnpCaA+/NDoSIQQ\n",
       "tmTBAjh1Ct56y+hIap5DlwQA0tL0dBLHj+ukIIRwbImJEBysJ4fz9DQ6mqqTkkAVdeyo1wX99FOj\n",
       "IxFCGO30aRg9Wo8MrosJoDocviQAuofQU0/p4eCyGL0QjqmsDB54ALp2rdvVQFISqIa77tINxB98\n",
       "YHQkQgij/PvfkJsLb75pdCS1S0oCvzt0CAYPhh9/1AvQCCEcx7ZtenLJXbugXTujo7k+Vi0JpKSk\n",
       "4Ovra365uLgQFhZW7pj8/HyCg4Px8vLCz8+P1NTUa547a9YsPDw8zPs2bdpU1fu0ultvhQkT4KWX\n",
       "jI5ECFGbsrL0dBDLl9f9BFAdFpcEysrKcHd3JykpiXaX/aSWLFnCwYMHCQsLIzExkdDQUBISEq56\n",
       "7uzZs2nevDnPPfdc5YHVckkAIC8PunSBmBjdY0gIYd9KS+G++/Tf+xtvGB2NddRYm0BcXBxeXl7l\n",
       "EgDAli1bCAoKAsDPz48jR46QnZ19zXNtsRbKxUXXBz79tMwpJIQjePNNKC6G2bONjsQ4FieB1atX\n",
       "M3bs2Cu2BwYGEhUVRVFRETExMWRlZZGenn7NcxcuXEjXrl2ZPHkyubm51Qzf+kJC9LeDTz4xOhIh\n",
       "RE3askVPCbFqFTRoYHQ0xrGoOqi4uBh3d3eSk5NxdXUtt6+wsJD58+ezbt06vL29SUlJYenSpXTv\n",
       "3r3Sc7OysnB1deXs2bO88MILlJaWsuwPa7WZTCZeffVV8/uAgAACAgKu934tsnOnHjJ++LCsNyCE\n",
       "PcrI0OsDrFypewfWZfHx8cTHx5vfz5492/qLyqxdu5ZFixZZ1IDbsWNHDhw4gLOzs0Xn7t+/n3Hj\n",
       "xnHgwIHygRnQJnC5SZOgVau63V9YCHGlkhK9QlhAAFz2PdNu1EibQFRUFGMqWU0hLy+P4uJiACIi\n",
       "IvD39zcngMrOzcjIAKCkpIRVq1bh4+NjccC1Zc4c3Vvg8GGjIxFCWNPs2br655VXjI7ENlyzJFBQ\n",
       "UECHDh1IS0ujefPmAISHhwMwbdo0EhMTmThxIk5OTvj4+LB48WKcfp+Ep6JzASZMmMC+ffto1KgR\n",
       "gwcPJjQ0FDc3t/KBGVwSAHjnHfj6a4iN1VNPCyHqtjVr9IJSe/bAHx45dkPWGLai4mLo0UOPJHzg\n",
       "AUNDEUJcpx079N/xV19Br15GR1NzZNoIK2rUSM8l/uyzcP680dEIIaorNVV39oiMtO8EUB2SBK7h\n",
       "nnvAx0dXDQkh6p7Tp+H++3Uj8P33Gx2N7ZHqIAscPapHFO7fDx4eRkcjhLDU+fMwdCjccQfMm2d0\n",
       "NLVD2gRqyMyZuki5apXRkQghLFFWpucEMpn03209B6n3kCRQQwoK9CRzn34KgwYZHY0Q4lpCQ+G7\n",
       "72DzZmjSxOhoao80DNeQZs30wLHp0/W0EkII27V4MURH65cjJYDqkCRQBY88Ai1a6KXnhBC2acMG\n",
       "PSBs40Y96l9cnVQHVdEPP+gh58nJ8h9MCFuzZw/cey+sWwf9+xsdjTGkTaAWPPUUKCXLUQphS375\n",
       "BQYMgIUL4c9/Njoa40gSqAVnzuhG4q+/1iOKhRDGys2FgQNh8mQ9uNORSRKoJYsXQ1QUxMfLvEJC\n",
       "GKm4WK8OdttteoS/o/89Su+gWjJ1Kpw9C599ZnQkQjgupfTfYvPmsGCBJIDqkJLAddi+HcaOhUOH\n",
       "dBdSIUTtmjVL9wKKj4ffJy92eFISqEUDB+qBY3PnGh2JEI5nxQr4+GPdE0gSQPVJSeA6nTypG4eT\n",
       "kqBTJ6OjEcIxxMXBo4/C1q3QpYvR0dgWKQnUMnd3eP55eO45oyMRwjEcOKCrYT//XBKANUgSsIJn\n",
       "n4WDB/ViFUKImnPqFAwbpnsBDR5sdDT2QZKAFTRponsmzJihu6sJIawvPx+CguAvf9GzgwrrkDYB\n",
       "K1FK/wdh1TSMAAAZFklEQVQdOlSqhoSwtpISvTRku3YQHi5dQa9GBosZKCVF9xg6cADatjU6GiHs\n",
       "g1L62//x47onUIMGRkdk26Rh2EDe3vDYY/CPfxgdiRD2Y9483fvus88kAdQEKQlY2dmzusfCmjVw\n",
       "++1GRyNE3bZ6tV4cJjERbr7Z6GjqBikJGOyGG/TgsenT9fJ2Qojq2bYNnn4a1q+XBFCTrpoEUlJS\n",
       "8PX1Nb9cXFwICwsrd0x+fj7BwcF4eXnh5+dHamrqNc+9dE737t0ZMWIE586dq6HbM8a4cbrYGhlp\n",
       "dCRC1E0pKTBypF7O1cfH6Gjsm8XVQWVlZbi7u5OUlES7du3M25csWcLBgwcJCwsjMTGR0NBQEhIS\n",
       "rnruiy++SOvWrXnxxReZN28eOTk5zP3D3At1tTrokt27dW+Gw4fBxcXoaISoO7KywM8PXnlFt7GJ\n",
       "qqmx6qC4uDi8vLzKJQCALVu2EBQUBICfnx9HjhwhOzv7qufGxMQQEhICQEhICNHR0RYHXFf06aMH\n",
       "tbz2mtGRCFF3FBbqL0+PPioJoLZYnARWr17N2LFjr9geGBhIVFQURUVFxMTEkJWVRXp6+lXPzczM\n",
       "xM3NDQA3NzcyMzOrG79Ne+MNPcHVoUNGRyKE7Sst1Q9/b2+9RrCoHRZ1uCouLmbdunXMmzfvin2j\n",
       "Ro0iPT0df39/vL296dOnD/Xr17foXNBFF1MlIz9mzZpl/ndAQAABAQGWhGsz2rTRRdqnn9arkMkA\n",
       "FyEq9/zzkJcH//d/8rdSFfHx8cTHx1f/AsoC0dHRKjAw0JJDlaenp8rPz7/qud7e3iojI0MppdSp\n",
       "U6eUt7f3FdexMDSbV1ysVNeuSq1ZY3QkQtiud9/Vfyc5OUZHUvdV9dlpUXVQVFQUYyqZrCMvL4/i\n",
       "3yfMiYiIwN/fH2dn56ueO3z4cCJ/7zoTGRlJcHBwNdJX3dCwIYSF6akkioqMjkYI27NmDfz733px\n",
       "mBYtjI7G8Vyzd1BBQQEdOnQgLS2N5s2bAxAeHg7AtGnTSExMZOLEiTg5OeHj48PixYtx+n2Fh4rO\n",
       "Bd1FdPz48Rw9ehQvLy9WrlxZLnFA3e8d9EcPPww9e+rqISGE9tVXMH48xMZC795GR2MfZO4gG3Xs\n",
       "mO4xtHevngRLCEf3xRfwxBMQHQ0DBhgdjf2QEcM2ytMTnnoK/vY3oyMRwngff6z/Hr76ShKA0aQk\n",
       "UIsKC6FrV702ah3r6CSE1Xz4IcyZo3vM3Xqr0dHYHykJ2DAnJ5g/X3cZLSkxOhohat/cufpvICFB\n",
       "EoCtkCRQy/78Z3B1hUWLjI5EiNqjFLz0kq4GSkiAjh2NjkhcItVBBjh8WK+P+sUXMGiQ0dEIUbPK\n",
       "yvTSq999p9sAWrc2OiL7JtVBdUCXLrBqle42KlNKCHtWUgKTJ+tecVu2SAKwRZIEDDJ0qB4gc//9\n",
       "8OuvRkcjhPUVF+sF4U+e1CUAmU3XNkkSMFBICEyapGcbtbMlFYSDKyyEBx/UJYF166BZM6MjEpWR\n",
       "NgGDKQVTpkBmph40I2uoirru7Fk9HXT79rB8ufyfrm3SJlDHmEyweLH+xjR9uk4KQtRVp0/rqs6u\n",
       "XfXKepIAbJ8kARvQsCF8/jns2AGVzLgthM3LyNCDIAMC9ICwevJ0qRMkT9uI5s1hwwa9rF779lDB\n",
       "+j1C2KxfftElgIkT9XgAWQ+g7pAkYENuvlkngrvuAnd38Pc3OiIhru3IEbj7bj1d+owZRkcjqkoK\n",
       "bDamWzeIioJHHoHkZKOjEeLqfvhBV/+8+qokgLpKkoANuvNOPb/K/ffrelYhbNHOnboE8O67uquz\n",
       "qJukOshGjRun61mDgmDrVt1mIISt+OYbGDVKdwENCjI6GnE9ZJyADVMKpk2DEyf0gBvpbidswYYN\n",
       "8Nhj8NlnMiW6LZJxAnbEZNJd7UCvwOTgOVHYgM8+01U/69ZJArAXkgRsXIMG+g9vzx54802joxGO\n",
       "7KOP4NlnYfNmuP12o6MR1iIVDHXA5WMIOnTQ7QVC1Kb33oN33tFtAbfcYnQ0wpokCdQRN90EGzfC\n",
       "kCF6PMGddxodkXAESsEbb+jFYLZt0wMZhX2RhuE6Jj5ejyHYskWPKRCipigFoaEQG6urgNq2NToi\n",
       "YQlpGLZzAQG6X3ZQkJ6nXYiaUFYGf/2r/tKxdaskAHt21SSQkpKCr6+v+eXi4kJYWFi5Y/Lz8wkO\n",
       "DsbLyws/Pz9SU1PN+woKCggJCcHX15euXbuyc+dOAGbNmoWHh4f5ups2baqBW7NfY8fq3kJBQXra\n",
       "XiGsqaREr3Vx6BD8979w441GRyRqksXVQWVlZbi7u5OUlES7du3M25csWcLBgwcJCwsjMTGR0NBQ\n",
       "EhISAAgJCcHf359JkyZRUlJCQUEBLi4uzJ49m+bNm/Pcc89VHphUB12VUjoRpKXB+vV6JlIhrlde\n",
       "HkyYABcvwn/+A05ORkckqqrGqoPi4uLw8vIqlwAAtmzZQtDvQwb9/Pw4cuQI2dnZ5OXlsW3bNib9\n",
       "Pp68QYMGuFy2vpw84K+PyQTvv68f/n/5i4whENdv+3bo2VNPXhgdLQnAUVicBFavXs3YCuY3DgwM\n",
       "JCoqiqKiImJiYsjKyiI9PZ20tDRcXV2ZOHEi3bp1Y+rUqRQVFZnPW7hwIV27dmXy5Mnk5uZa524c\n",
       "TIMGsHo17N8Pr79udDSiriopgX/+Ex5+GMLC9ADFRo2MjkrUFouqg4qLi3F3dyc5ORlXV9dy+woL\n",
       "C5k/fz7r1q3D29ublJQUli5dSnFxMf369WPt2rUMHTqUadOmMXToUCZMmEBWVhaurq6cPXuWF154\n",
       "gdLSUpYtW1Y+MJOJV1991fw+ICCAABmiWKFff4UBA/RMjiEhRkcj6pLUVD3uxMUFVqyQBuC6KD4+\n",
       "nvj4ePP72bNnV62mRVkgOjpaBQYGWnKo8vT0VPn5+SojI0O1bt3avH3jxo1q9OjRVxy/b98+1a1b\n",
       "tyu2Wxia+F1yslJt2ii1ebPRkYi6oKxMqRUrlGrdWql331WqtNToiIS1VPXZaVF1UFRUFGPGjKlw\n",
       "X15eHsXFxQBERETg7++Ps7Mzbdu2pXPnzuzcuZOysjI2bNjA0KFDAcj4fX7kkpISVq1ahY+Pj+VZ\n",
       "S1To1lt1Q97YsXqOdyEqk5MDY8bAW2/p3j8zZshSkI7smr/6goIC4uLi+POf/2zeFh4eTnh4OADJ\n",
       "ycn4+Pjg6+vLtm3b+PDSjGdAZGQkM2bM4JZbbuHkyZOMHj0agNDQULp3707//v25ePEiCxYssPZ9\n",
       "OaRBg2DhQhg2DNLTjY5G2KKEBN346+oKu3ZB9+5GRySMJiOG7dBbb8HKlXqY/2UdsoQDu3gRZs3S\n",
       "8/8vXaoXLBL2qarPTkkCdkgpeOopvfbrhg3S08PR/fQTPPqo/vb/0Ufg5mZ0RKImybQRApNJd/Vr\n",
       "2hQef1zGEDgqpfRDf8AAPQBs/XpJAOJKUhKwYwUFetbR++6D2bONjkbUpjNn9Kp0KSmwapVMNuhI\n",
       "pCQgzJo10ytAffKJXri+rMzoiERt+OYb3fjr4QFJSZIAxNVJScABpKbqOuGmTXWjoJeX0RGJmlBc\n",
       "rEf+rlwJy5bBvfcaHZEwgpQExBW8vODbb+GBB/SygAsWQGmp0VEJa0pJ0XX/P/4I+/ZJAhCWkyTg\n",
       "IOrXh+eegx07YO1aGDgQkpONjkpcL6UgIkL/PidPhpgY3QtICEtJdZADKiuDJUtg5kx45hl48UWZ\n",
       "irouOn0apk6Fo0d142/XrkZHJGyBVAeJa6pXT08/vWePnj64Xz/Yu9foqERVxMVBjx7QqRPs3CkJ\n",
       "QFSflAQcnFJ6EfEXXtBjCmbOhMaNjY5KVObCBXjlFYiK0qN/777b6IiErZGSgKgSk0lPP71/v24j\n",
       "8PXV7QbC9hw6BH5+egTwvn2SAIR1SBIQANx0E3zxhR5UNmKEbkQuLDQ6KgG6tLZ4MQwerKvx1qyB\n",
       "1q2NjkrYC0kCwsxkgpEj4cAByMoCHx898EgYQyndZnP//boH0LZtusrOZDI6MmFPpE1AVGr9er2Y\n",
       "/bBhMG8e3HCD0RE5hosX4fPP9XiO3Fw93//jj8tEgMIy0iYgrGbYMF0qKCnRUw/ExhodkX07cwbm\n",
       "zoWOHfU3/5kz4fBhPSOsJABRU6QkICwSF6f7pA8erL+h3nij0RHZjyNH4L33dF//Bx6AZ5/VDfRC\n",
       "VIeUBESNGDpUlwpcXHSp4MsvjY6oblMKtmzRD/2BA6FlS9076+OPJQGI2iUlAVFl334LkybpwUoL\n",
       "F8oc9VVx4YLu4//uu3rCt2eegXHjwMnJ6MiEvZCSgKhxd9yh+6l36qTXqP3kE1m45lqys+G118DT\n",
       "UyeBuXPh4EHd4CsJQBhJSgLiuuzerUsF7dvrvuweHkZHZFt+/FF/6//Pf+Chh/Q3f5nfX9QkKQmI\n",
       "WtWnj04EffvquuyICCkVKAWbNkFgoG5LaddOT/W8dKkkAGF7pCQgrObAAV0quOEGeP11nSAcqWtj\n",
       "UZFe0OXdd/WsrM8+C2PGyFxMonZV9dkpSUBYVUmJ7u74ySd6jps+fXTvl4ED9bw3Li5GR2h9GRnw\n",
       "wQd6eu7bb9cP/yFDZGSvMIZVq4NSUlLw9fU1v1xcXAgLCyt3TH5+PsHBwXh5eeHn50dqaqp5X0FB\n",
       "ASEhIfj6+tK1a1d2/D4z2aVzunfvzogRIzh37lxV7lHYsAYN4Pnn9dTUJ0/CP/6hH4bz5oG7u177\n",
       "9qmnYPVqSE83Otrrs2+fnnyva1fIydFTPKxbB3feKQlA1B0WlwTKyspwd3cnKSmJdu3ambcvWbKE\n",
       "gwcPEhYWRmJiIqGhoSQkJAAQEhKCv78/kyZNoqSkhIKCAlxcXHjxxRdp3bo1L774IvPmzSMnJ4e5\n",
       "c+eWD0xKAnanuFgnh+3b9evbb3XPmEslhYED9QO1ng22VCkFmZmQlqYHd61YoUs6Tz2le/jI4Dlh\n",
       "K2qsOujrr7/mtddeY/v27eW2jx49mscee4zAwEAA2rZty4EDB2jUqBG+vr4cPXr0imt16dKFrVu3\n",
       "4ubmxq+//kpAQACHDx++rhsRdY9S+kF6KSls3w6//abXyr3jDp0U+vaFJk1qJ56cHDh2TD/o//g6\n",
       "dgyaNdNTOnh6QnCwnmxPVmQTtqaqz84Glh64evVqxo4de8X2wMBAoqKiGDx4MJs3byYrK4v09HRM\n",
       "JhOurq5MnDiR3bt34+fnR1hYGE2bNiUzMxO330cYubm5kZmZaXHAwn6YTHDLLfo1aZLelpmpSwjb\n",
       "t+tqpR9/1FVIl0oKAwZAq1bV+7zCwsof8mlpUFqqH/KXXn/6E9xzz/8e/M2bW+vOhbAdFpUEiouL\n",
       "cXd3Jzk5Gdc/rGJdWFjI/PnzWbduHd7e3qSkpLB06VKKi4vp168fa9euZejQoUybNo2hQ4cyYcIE\n",
       "WrZsSU5OjvkaN954I2fOnCkfmMnEq6++an4fEBBAQEDAdd6uqGsKCvTyiZdKCjt26C6Xl1cheXrq\n",
       "hHLxIpw4UflDPjcXOnTQx1/+sL/0atVK6vJF3RMfH098fLz5/ezZs61fHbR27VoWLVrEpk2brnnB\n",
       "jh07cuDAAc6dO4ePjw/Z2dkAxMbG8vHHHxMVFUWXLl2Ij4+nbdu2ZGRkMGTIEKkOEhYpKYEffvhf\n",
       "m8K2bXp7o0a6l07bthU/4Dt21Avn2GJ7gxDWVCPVQVFRUYwZM6bCfXl5eTRt2pRGjRoRERGBv78/\n",
       "zs7OODs707lzZ3bu3Enfvn3ZsGEDQ4cOBWD48OFERkYSGhpKZGQkwcHBFgcsHFuDBtCrl349/bRu\n",
       "Vzh2DMrKdAnBkcYlCGEN1ywJFBQU0KFDB9LS0mj+e6VoeHg4ANOmTSMxMZGJEyfi5OSEj48Pixcv\n",
       "xun3yVCOHDnChAkT+O233/Dx8eGTTz6hWbNm5OfnM378eI4ePYqXlxcrV67E2dm5fGBSEhBCiCqT\n",
       "wWJCCOHAZO4gIYQQFpMkIIQQDkySgBBCODBJAkII4cAkCQghhAOTJCCEEA5MkoAQQjgwSQJCCOHA\n",
       "JAkIIYQDkyQghBAOTJKAEEI4MEkCQgjhwCQJCCGEA5MkIIQQDkySgBBCODBJAkII4cAkCQghhAOT\n",
       "JCCEEA5MkoAQQjgwSQJCCOHAJAkIIYQDkyQghBAOTJKAEEI4sKsmgZSUFHx9fc0vFxcXwsLCyh2T\n",
       "n59PcHAwXl5e+Pn5kZqaat7n6elJ9+7d8fX1pV+/fubts2bNwsPDw3zdTZs2Wfm2bF98fLzRIdQo\n",
       "ub+6zZ7vz57vrTqumgS8vb3Zu3cve/fuZc+ePTg5OTFixIhyx0RFRdG+fXtSU1N55513eOyxx8z7\n",
       "TCYT8fHx7N27l6SkpHLbn3vuOfO17733Xivflu2z9/+Icn91mz3fnz3fW3VYXB0UFxeHl5cX7dq1\n",
       "K7d9y5YtBAUFAeDn58eRI0fIzs4271dKVXi9yrYLIYSoPRYngdWrVzN27NgrtgcGBhIVFUVRUREx\n",
       "MTFkZWWRnp4O6G/8d955J76+vkRERJQ7b+HChXTt2pXJkyeTm5t7nbchhBCiWpQFLly4oFq3bq2y\n",
       "srKu2FdQUKBee+011bdvXzVu3DjVt29ftX//fqWUUqdOnVJKKZWcnKw8PT1VQkKCUkqpzMxMVVZW\n",
       "pnJzc9XUqVPVpEmTrrguIC95yUte8qrGqypMyoJ6mbVr17Jo0SKLGnA7duzIgQMHcHZ2Lrf9ueee\n",
       "w93dneeff77c9v379zNu3DgOHDhwzWsLIYSwLouqg6KiohgzZkyF+/Ly8iguLgYgIiICf39/nJ2d\n",
       "KSwsJD8/H4Ds7Gw2btyIj48PABkZGQCUlJSwatUq83YhhBC165olgYKCAjp06EBaWhrNmzcHIDw8\n",
       "HIBp06aRmJjIxIkTcXJywsfHh8WLF+Pk5ERaWpq5J1GrVq145JFHmDZtGgATJkxg3759NGrUiMGD\n",
       "BxMaGoqbm1tN3qcQQoiKVKnyqBZs3bpV+fr6Kh8fHxUWFmZ0OFZ1/PhxFRAQoLp27ar8/f3V8uXL\n",
       "jQ6pRpSUlKiePXuqYcOGGR2K1Z07d05NmDBB9ezZU916660qMTHR6JCsasmSJcrPz0/16tVLzZgx\n",
       "w+hwrstjjz2m2rRpo7p162bedvbsWfXggw8qHx8fFRwcrPLz8w2M8PpUdH9/+9vfVJcuXZSvr6+a\n",
       "MWOGys3NveZ1bCoJlJSUKC8vL5WWlqaKi4tVjx49VHJystFhWU1GRobau3evUkqp7Oxs5ebmZlf3\n",
       "d8n8+fPV2LFj1QMPPGB0KFY3YcIEtWzZMqWUUhcvXrToj6yuOH36tPL09FTnzp1TpaWl6r777lOb\n",
       "Nm0yOqxqS0hIUN9//325h+QLL7yg5s2bp5RSau7cuSo0NNSo8K5bRff39ddfq9LSUlVaWqqmTJli\n",
       "0f3Z1LQRSUlJdO7cGU9PTxo2bMjo0aNZu3at0WFZTdu2benZsycArVu3pm/fvpw6dcrgqKwrPT2d\n",
       "jRs3MmXKFLsbC5KXl8e2bduYNGkSAA0aNMDFxcXgqKynadOmKKXIy8ujqKiIwsJCWrZsaXRY1TZo\n",
       "0KAr4o+JiSEkJASAkJAQoqOjjQjNKiq6v7vvvpt69epRr149AgMDzd31r8amksDJkyfLDUbz8PDg\n",
       "5MmTBkZUc37++Wd+/PFH+vfvb3QoVvXss8/y1ltvUa+eTf3Xsoq0tDRcXV2ZOHEi3bp1Y+rUqRQV\n",
       "FRkdltU0bdqURYsW4enpSdu2bbnjjjvKTfdiDzIzM83tj25ubmRmZhocUc2JiIjgwQcfvOZxNvWX\n",
       "ajKZjA6hVpw7d47Ro0ezYMECmjVrZnQ4VrN+/XratGmDr6+v3ZUCQPdm27VrFw899BC7du3iwoUL\n",
       "fP7550aHZTXZ2dk88cQTJCcnc+zYMRITE9mwYYPRYdUYk8lkt8+cN954g+bNmzNy5MhrHmtTScDd\n",
       "3Z0TJ06Y3584cQIPDw8DI7K+ixcv8tBDDzFu3DiLsnRd8t133xETE0PHjh0ZM2YMW7ZsYcKECUaH\n",
       "ZTUeHh60atWKBx54gKZNmzJmzBhiY2ONDstqkpKS6N+/P507d6ZVq1aMHDmShIQEo8OyKjc3N379\n",
       "9VdAd1Vv06aNwRFZ34oVK9i4cSOffPKJRcfbVBLo06cPP/30E8eOHaO4uJj/+7//Y/jw4UaHZTVK\n",
       "KSZPnsxtt93GM888Y3Q4Vvfmm29y4sQJ0tLSWL16NXfeeScff/yx0WFZTdu2bencuTM7d+6krKyM\n",
       "DRs2MHToUKPDsppBgwaxe/duzpw5w4ULF4iNjeWee+4xOiyrGj58OJGRkQBERkYSHBxscETWtWnT\n",
       "Jt566y1iYmJo0qSJZSfVVMt1dcXHx6uePXuqbt26qffee8/ocKxq27ZtymQyqR49eqiePXuqnj17\n",
       "qtjYWKPDqhHx8fF22TsoJSVF3X777crLy0sFBwerc+fOGR2SVS1fvlwNHjxY9enTR73yyiuqtLTU\n",
       "6JCqbfTo0eqmm25SjRo1Uh4eHuqjjz6yqy6il+6vYcOGysPDQy1btkx17txZtW/f3vx8eeKJJ655\n",
       "HYumjRBCCGGfbKo6SAghRO2SJCCEEA5MkoAQQjgwSQJCCOHAJAkIIYQDkyQghBAO7P8B52NPrpOX\n",
       "ADEAAAAASUVORK5CYII=\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x473eab0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_grad_descent(.1295,13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last graph is an example of what would happen if we set alpha too large.   In this case gradient descent it taking steps that are too large.   When that happens, $J(\\theta)$ will 'miss' the minimum and will begin increasing as iterations increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Prediction\n",
    "\n",
    "Now that we have some learned weights for our function, we can use them to make a prediction.  In testData we've created three hypothetical houses.  The first house has 2 bedrooms and 2000 square feet.   The second has 3 and 2200, and the last has 5 bedrooms and 4000 square feet.   \n",
    "\n",
    "We'll need to scale this data and an intercept term, as before in training.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = .01\n",
    "iterations = 100\n",
    "#np.shape[1] is the number of features, so we need an equal # of thetas\n",
    "theta = np.zeros((X.shape[1],1))\n",
    "theta, J_history = gradient_descent(X,y,theta,alpha, iterations)\n",
    "\n",
    "testData = np.array([[2,2000],\n",
    "                     [3,2200],\n",
    "                     [5,4000]])\n",
    "\n",
    "\n",
    "\n",
    "testData, muTest, sigmaTest = scaleData(testData)\n",
    "\n",
    "\n",
    "#Add a column of ones to X (intercept term)\n",
    "m = testData.shape[0]\n",
    "it = np.ones(shape=(m, 1))\n",
    "testData = np.append(it,testData,1)\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've done that we have testData, a 3x3 matrix,  and $\\theta$, a 3x1 column vector.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print testData\n",
    "print theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we multiply, we're left with a 3x1 column vector of predicteded house prices.  Lastly, we have to multiple the answer by the standard deviation of house price and add back the mean, to 'unscale' the result.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predicted house prices, unscaled.\n",
    "print (testData.dot(theta)*sigma[0]+mu[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#What's Next?\n",
    "-Logistic Regression\n",
    "\n",
    "-L1 / L2 Regularization\n",
    "\n",
    "-Minibatch and online algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
